{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import librosa\n",
    "import itertools\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings  \n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG = './data/images_original/'\n",
    "#IMG = './dataset/'\n",
    "img_dataset = []\n",
    "genre_target = []\n",
    "genres = {}\n",
    "classes = []\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(IMG):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "        img_dataset.append(filename)\n",
    "        genre = filename.split('\\\\')[0].split('/')[-1]\n",
    "        genre_target.append(genre)\n",
    "        \n",
    "        if(genre not in genres):\n",
    "            classes.append(genre)\n",
    "            genres[genre] = i\n",
    "            i+=1\n",
    "\n",
    "img = cv2.imread(img_dataset[0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_borders(img,x1=35,x2=252,y1=54,y2=389):\n",
    "    cropped = img[x1:x2,y1:y2]\n",
    "    return cropped\n",
    "\n",
    "def get_y():\n",
    "    '''Convierte los generos en un array de targets y'''\n",
    "    y = []\n",
    "    for genre in genre_target:\n",
    "        n = genres[genre]\n",
    "        y.append(n)\n",
    "    return np.array(y)\n",
    "\n",
    "def get_x(shape=[999,217,335], flag=0):\n",
    "    x = np.empty(shape, np.uint8)\n",
    "    for i in range(len(img_dataset)):\n",
    "        img = cv2.imread(img_dataset[i],flag)\n",
    "        img = crop_borders(img)\n",
    "        #img = crop_borders(img,x1=50,x2=252,y1=54,y2=244)\n",
    "        x[i] = img\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_dataset[0],0)\n",
    "img = crop_borders(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 217, 335) (999,)\n"
     ]
    }
   ],
   "source": [
    "X = get_x(shape=[999,img.shape[0], img.shape[1]])\n",
    "y = get_y()\n",
    "\n",
    "m = len(y)\n",
    "num_labels = 10 #estilos de musica diferente\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to plot confusion matrix -- from Scikit-learn website\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(genres))\n",
    "    plt.xticks(tick_marks, genres, rotation=45)\n",
    "    #plt.yticks(tick_marks, genres) #imprime mal en jupyter\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 217, 335) (999,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 217, 335)\n",
      "(749, 217, 335, 1) (250, 217, 335, 1) (749, 10) (250, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "print(X_test.shape)\n",
    "X_test = X_test.reshape(250,217,335,1)\n",
    "X_train = X_train.reshape(749,217,335,1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram for train and test \n",
    "values, count = np.unique(np.argmax(y_train, axis=1), return_counts=True)\n",
    "plt.bar(values, count)\n",
    "\n",
    "values, count = np.unique(np.argmax(y_test, axis=1), return_counts=True)\n",
    "plt.bar(values, count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, n_filters, pool_size=(2, 2)):\n",
    "    x = Conv2D(n_filters, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=pool_size, strides=pool_size)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "def create_model(input_shape, num_genres):\n",
    "    inpt = Input(shape=input_shape)\n",
    "    x = conv_block(inpt, 16)\n",
    "    x = conv_block(x, 32)\n",
    "    x = conv_block(x, 64)\n",
    "    x = conv_block(x, 128)\n",
    "    x = conv_block(x, 256)\n",
    "    \n",
    "    # Global Pooling and MLP\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu', \n",
    "              kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    predictions = Dense(num_genres, \n",
    "                        activation='softmax', \n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "    \n",
    "    model = Model(inputs=inpt, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\willw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = create_model(X_train[0].shape, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceLROnPlat = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.95,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=2,\n",
    "    min_lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "Epoch 2/150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-63e31e85e938>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     callbacks=[reduceLROnPlat])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=150,\n",
    "    verbose=50,\n",
    "    callbacks=[reduceLROnPlat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"val_loss = {:.3f} and val_acc = {:.3f}\".format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist.history['accuracy'], label='train')\n",
    "plt.plot(hist.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist.history['loss'], label='train')\n",
    "plt.plot(hist.history['val_loss'], label='validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(model.predict(X_test), axis = 1)\n",
    "y_orig = np.argmax(y_test, axis = 1)\n",
    "cm = confusion_matrix(preds, y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = OrderedDict(sorted(genres.items(), key=lambda t: t[1])).keys()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, keys, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('../models/custom_cnn_2d.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_pred = {}\n",
    "for k,v in genres.items():\n",
    "    genres_pred[v] = k\n",
    "\n",
    "files = os.listdir('./test_songs/images/')\n",
    "songs = [ './test_songs/images/'+img for img in files if img.split('.')[-1] == 'png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in songs:\n",
    "    img = cv2.imread(song)\n",
    "    mel = img.ravel()\n",
    "    print(song[20:-4],\"-->\",'PREDICTION','\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
